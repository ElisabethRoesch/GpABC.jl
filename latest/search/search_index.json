{
    "docs": [
        {
            "location": "/", 
            "text": "#\n\n\nGpAbc.AbstractGPKernel\n \n \nType\n.\n\n\nAbstractGPKernel\n\n\n\n\nAbstract kernel type. User-defined kernels should derive from it.\n\n\nImplementations have to provide methods for \nget_hyperparameters_size\n and \ncovariance\n. Methods for \ncovariance_training\n, \ncovariance_diagonal\n and \ncovariance_grad\n are optional.\n\n\nsource\n\n\n#\n\n\nGpAbc.GPModel\n \n \nType\n.\n\n\nGPModel\n\n\n\n\nThe main type that is used by most functions withing the package.\n\n\nAll data matrices are row-major.\n\n\nFields\n\n\n\n\nkernel::AbstractGPKernel\n: the kernel\n\n\ngp_training_x::AbstractArray{Float64, 2}\n: training \nx\n. Size: $n \\times d$.\n\n\ngp_training_y::AbstractArray{Float64, 2}\n: training \ny\n. Size: $n \\times 1$.\n\n\ngp_test_x::AbstractArray{Float64, 2}\n: test \nx\n.  Size: $m \\times d$.\n\n\ngp_hyperparameters::AbstractArray{Float64, 1}\n: kernel hyperparameters, followed by standard deviation of intrinsic noise $\\sigma_n$, which is always the last element in the array.\n\n\ncache::HPOptimisationCache\n: cache of matrices that can be re-used between calls to \ngp_loglikelihood\n and \ngp_loglikelihood_grad\n\n\n\n\nsource\n\n\n#\n\n\nGpAbc.GPModel\n \n \nType\n.\n\n\nGPModel(training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},\n        training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},\n        kernel::AbstractGPKernel\n        [,test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0)])\n\n\n\n\nConstructor of \nGPModel\n that allows the kernel to be specified. Arguments that are passed as 1-d vectors will be reshaped into 2-d.\n\n\nsource\n\n\n#\n\n\nGpAbc.GPModel\n \n \nType\n.\n\n\nGPModel(training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},\n        training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}\n        [,test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0)])\n\n\n\n\nDefault constructor of \nGPModel\n, that will use \nSquaredExponentialIsoKernel\n. Arguments that are passed as 1-d vectors will be reshaped into 2-d.\n\n\nsource\n\n\n#\n\n\nGpAbc.GPModel\n \n \nMethod\n.\n\n\nGPModel(;training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),\n    training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),\n    test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),\n    kernel::AbstractGPKernel=SquaredExponentialIsoKernel(),\n    gp_hyperparameters::AbstractArray{Float64, 1}=Array{Float64}(0))\n\n\n\n\nConstructor of \nGPModel\n with explicit arguments. Arguments that are passed as 1-d vectors will be reshaped into 2-d.\n\n\nsource\n\n\n#\n\n\nGpAbc.MaternArdKernel\n \n \nType\n.\n\n\nMaternArdKernel \n: AbstractGPKernel\n\n\n\n\nMat\u00e9rn kernel with distinct length scale for each dimention, $l_k$. Parameter $\\nu$ (nu) is passed in constructor. Currently, only values of $\\nu=1$, $\\nu=3$ and $\\nu=5$ are supported.\n\n\n\n\n\n\\begin{aligned}\nK_{\\nu=1}(r) &= \\sigma_f^2e^{-\\sqrt{r}}\\\\\nK_{\\nu=3}(r) &= \\sigma_f^2(1 + \\sqrt{3r})e^{-\\sqrt{3r}}\\\\\nK_{\\nu=5}(r) &= \\sigma_f^2(1 + \\sqrt{3r} + \\frac{5}{3}r)e^{-\\sqrt{5r}}\\\\\nr_{ij} &= \\sum_{k=1}^d\\frac{(x_{ik}-z_{jk})^2}{l_k^2}\n\\end{aligned}\n\n\n\n\n\nHyperparameters\n\n\nThe length of hyperparameters array for this kernel depends on the dimensionality of the data. Assuming each data point is a vector in a $d$-dimensional space, this kernel needs $d+1$ hyperparameters, in the following order:\n\n\n\n\n$\\sigma_f$: the signal standard deviation\n\n\n$l_1, \\ldots, l_d$: the length scales for each dimension\n\n\n\n\nsource\n\n\n#\n\n\nGpAbc.MaternIsoKernel\n \n \nType\n.\n\n\nMaternIsoKernel \n: AbstractGPKernel\n\n\n\n\nMat\u00e9rn kernel with uniform length scale across all dimensions, $l$. Parameter $\\nu$ (nu) is passed in constructor. Currently, only values of $\\nu=1$, $\\nu=3$ and $\\nu=5$ are supported.\n\n\n\n\n\n\\begin{aligned}\nK_{\\nu=1}(r) &= \\sigma_f^2e^{-\\sqrt{r}}\\\\\nK_{\\nu=3}(r) &= \\sigma_f^2(1 + \\sqrt{3r})e^{-\\sqrt{3r}}\\\\\nK_{\\nu=5}(r) &= \\sigma_f^2(1 + \\sqrt{3r} + \\frac{5}{3}r)e^{-\\sqrt{5r}}\\\\\nr_{ij} &= \\sum_{k=1}^d\\frac{(x_{ik}-z_{jk})^2}{l^2}\n\\end{aligned}\n\n\n\n\n\nHyperparameters\n\n\nHyperparameters vector for this kernel must contain two elements, in the following order:\n\n\n\n\n$\\sigma_f$: the signal standard deviation\n\n\n$l$: the length scale\n\n\n\n\nsource\n\n\n#\n\n\nGpAbc.SquaredExponentialArdKernel\n \n \nType\n.\n\n\nSquaredExponentialArdKernel \n: AbstractGPKernel\n\n\n\n\nSquared exponential kernel with distinct length scale for each dimention, $l_k$.\n\n\n\n\n\nK(r) = \\sigma_f^2 e^{-r/2}; r_{ij} = \\sum_{k=1}^d\\frac{(x_{ik}-z_{jk})^2}{l_k^2}\n\n\n\n\n\nHyperparameters\n\n\nThe length of hyperparameters array for this kernel depends on the dimensionality of the data. Assuming each data point is a vector in a $d$-dimensional space, this kernel needs $d+1$ hyperparameters, in the following order:\n\n\n\n\n$\\sigma_f$: the signal standard deviation\n\n\n$l_1, \\ldots, l_d$: the length scales for each dimension\n\n\n\n\nsource\n\n\n#\n\n\nGpAbc.SquaredExponentialIsoKernel\n \n \nType\n.\n\n\nSquaredExponentialIsoKernel \n: AbstractGPKernel\n\n\n\n\nSquared exponential kernel with uniform length scale across all dimensions, $l$.\n\n\n\n\n\nK(r) = \\sigma_f^2 e^{-r/2}; r_{ij} = \\sum_{k=1}^d\\frac{(x_{ik}-z_{jk})^2}{l^2}\n\n\n\n\n\nHyperparameters\n\n\nHyperparameters vector for this kernel must contain two elements, in the following order:\n\n\n\n\n$\\sigma_f$: the signal standard deviation\n\n\n$l$: the length scale\n\n\n\n\nsource\n\n\n#\n\n\nGpAbc.ExponentialArdKernel\n \n \nMethod\n.\n\n\nExponentialArdKernel\n\n\n\n\nAlias for \nMaternArdKernel\n(1)\n\n\nsource\n\n\n#\n\n\nGpAbc.ExponentialIsoKernel\n \n \nMethod\n.\n\n\nExponentialIsoKernel\n\n\n\n\nAlias for \nMaternIsoKernel\n(1)\n\n\nsource\n\n\n#\n\n\nGpAbc.covariance\n \n \nMethod\n.\n\n\ncovariance(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},\n    x::AbstractArray{Float64, 2}, z::AbstractArray{Float64, 2})\n\n\n\n\nReturn the covariance matrix. Should be overridden by kernel implementations.\n\n\nArguments\n\n\n\n\nker\n: The kernel object. Implementations must override with their own subtype.\n\n\nlog_theta\n: natural logarithm of hyperparameters.\n\n\nx, z\n: Input data, reshaped into 2-d arrays. \nx\n must have dimensions $n \\times d$; \nz\n must have dimensions $m \\times d$.\n\n\n\n\nReturn\n\n\nThe covariance matrix, of size $n \\times m$.\n\n\nsource\n\n\n#\n\n\nGpAbc.covariance_diagonal\n \n \nMethod\n.\n\n\ncovariance_diagonal(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},\n    x::AbstractArray{Float64, 2})\n\n\n\n\nThis is a speedup version of \ncovariance\n, which is invoked if the caller is not interested in the entire covariance matrix, but only needs the variance, i.e. the diagonal of the covariance matrix.\n\n\nDefault method just returns \ndiag(covariance(...))\n, with \nx === z\n. Kernel implementations can optionally override it to achieve betrer performance, by not computing the non diagonal elements of covariance matrix.\n\n\nSee \ncovariance\n for description of arguments.\n\n\nReturn\n\n\nThe 1-d array of variances, of size \nsize(x, 1)\n.\n\n\nsource\n\n\n#\n\n\nGpAbc.covariance_grad\n \n \nMethod\n.\n\n\ncovariance_grad(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},\n    x::AbstractArray{Float64, 2}, R::AbstractArray{Float64, 2})\n\n\n\n\nReturn the gradient of the covariance function with respect to logarigthms of hyperparameters, based on the provided direction matrix.\n\n\nThis function can be optionally overridden by kernel implementations. If the gradient function is not provided, \ngp_train\n will fail back to \nNelderMead\n algorithm by default.\n\n\nArguments\n\n\n\n\nker\n: The kernel object. Implementations must override with their own subtype.\n\n\nlog_theta\n:  natural logarithm of hyperparameters\n\n\nx\n: Training data, reshaped into a 2-d array. \nx\n must have dimensions $n \\times d$.\n\n\nR\n the directional matrix, $n \\times n$\n\n\n\n\n\n\n\nR = \\frac{1}{\\sigma_n^2}(\\alpha * \\alpha^T - K^{-1}); \\alpha = K^{-1}y\n\n\n\n\n\nReturn\n\n\nA vector of size \nlength(log_theta)\n, whose $j$'th element is equal to\n\n\n\n\n\ntr(R \\frac{\\partial K}{\\partial \\eta_j})\n\n\n\n\n\nsource\n\n\n#\n\n\nGpAbc.covariance_training\n \n \nMethod\n.\n\n\ncovariance_training(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},\n    training_x::AbstractArray{Float64, 2})\n\n\n\n\nThis is a speedup version of \ncovariance\n, which is only called during traing sequence. Intermediate matrices computed in this function for particular hyperparameters can be cached and reused subsequently, either in this function or in \ncovariance_grad\n\n\nDefault method just delegates to \ncovariance\n with \nx === z\n. Kernel implementations can optionally override it for betrer performance.\n\n\nSee \ncovariance\n for description of arguments and return values.\n\n\nsource\n\n\n#\n\n\nGpAbc.get_hyperparameters_size\n \n \nMethod\n.\n\n\nget_hyperparameters_size(kernel::AbstractGPKernel, training_data::AbstractArray{Float64, 2})\n\n\n\n\nReturn the number of hyperparameters for used by this kernel on this training data set. Should be overridden by kernel implementations.\n\n\nsource\n\n\n#\n\n\nGpAbc.gp_loglikelihood\n \n \nMethod\n.\n\n\ngp_loglikelihood(theta::AbstractArray{Float64, 1}, gpm::GPModel)\n\n\n\n\nsource\n\n\n#\n\n\nGpAbc.gp_loglikelihood\n \n \nMethod\n.\n\n\ngp_loglikelihood(gpm::GPModel)\n\n\n\n\nCompute the log likelihood function, based on the kernel and training data specified in \ngpm\n.\n\n\n\n\n\nlog p(y \\vert X, \\theta) = - \\frac{1}{2}(y^TK^{-1}y + log \\vert K \\vert + n log 2 \\pi)\n\n\n\n\n\nsource\n\n\n#\n\n\nGpAbc.gp_regression\n \n \nMethod\n.\n\n\ngp_regression(gpm::GPModel; \noptional keyword arguments\n)\n\n\n\n\nRun the Gaussian Process Regression.\n\n\nArguments\n\n\n\n\ngpm\n: the \nGPModel\n, that contains the training data (x and y), the kernel, the hyperparameters and the test data for running the regression.\n\n\ntest_x\n: if specified, overrides the test x in \ngpm\n. Size $m \\times d$.\n\n\nlog_level::Int\n (optional): log level. Default is \n0\n, which is no logging at all. \n1\n makes \ngp_regression\n print basic information to standard output.\n\n\nfull_covariance_matrix::Bool\n (optional): whether we need the full covariance matrix, or just the variance vector. Defaults to \nfalse\n (i.e. just the variance).\n\n\nbatch_size::Int\n (optional): If \nfull_covariance_matrix\n is set to \nfalse\n, then the mean and variance vectors will be computed in batches of this size, to avoid allocating huge matrices. Defaults to 1000.\n\n\nobservation_noise::Bool\n (optional): whether the observation noise (with variance $\\sigma_n^2$) should be included in the output variance. Defaults to \ntrue\n.\n\n\n\n\nReturn\n\n\nA tuple of \n(mean, var)\n. \nmean\n is a mean vector of the output multivariate Normal distribution, of size $m$. \nvar\n is either the covariance matrix of size $m \\times m$, or a variance vector of size $m$, depending on \nfull_covariance_matrix\n flag.\n\n\nsource\n\n\n#\n\n\nGpAbc.gp_regression\n \n \nMethod\n.\n\n\ngp_regression(test_x::Union{AbstractArray{Float64, 1}, AbstractArray{Float64, 2}},\n    gpem::GPModel; \noptional keyword arguments\n)\n\n\n\n\nsource\n\n\n#\n\n\nGpAbc.gp_train\n \n \nMethod\n.\n\n\ngp_train(gpm::GPModel; \noptional keyword arguments\n)\n\n\n\n\nFind Maximum Likelihood Estimate of Gaussian Process hyperparameters by maximising \ngp_loglikelihood\n, using \nOptim\n package.\n\n\nArguments\n\n\n\n\ngpm\n: the \nGPModel\n, that contains the training data (x and y), the kernel and the starting hyperparameters that will be used for optimisation.\n\n\noptimisation_solver_type::Type{\n:Optim.Optimizer}\n (optional): the solver to use. If not given, then \nConjugateGradient\n will be used for kernels that have gradient implementation, and \nNelderMead\n will be used for those that don't.\n\n\nhp_lower::AbstractArray{Float64, 1}\n (optional): the lower boundary for box optimisation. Defaults to $e^{-10}$ for all hyperparameters.\n\n\nhp_upper::AbstractArray{Float64, 1}\n (optional): the upper boundary for box optimisation. Defaults to $e^{10}$ for all hyperparameters.\n\n\nlog_level::Int\n (optional): log level. Default is \n0\n, which is no logging at all. \n1\n makes \ngp_train\n print basic information to standard output. \n2\n switches \nOptim\n logging on, in addition to \n1\n.\n\n\n\n\nReturn\n\n\nThe list of all hyperparameters, including the standard deviation of the measurement noise $\\sigma_n$. Note that after this function returns, the hyperparameters of \ngpm\n will be set to the optimised value, and there is no need to call \nset_hyperparameters\n once again.\n\n\nsource\n\n\n#\n\n\nGpAbc.rejection_abc\n \n \nMethod\n.\n\n\ntest_prior - A no-argument function, returns a sample from test prior, that is ready to be fed into summary_statistics_function. Returns an array of size (n, d), n rows and d columns, where n is the number of points, and d is the dimentionality of the data\n\n\nsummary_statistics_function - A function that takes in a test sample, and returns a 1D vector of summary statistics. In our case this returns a vector of norms to the observed data, as emulated by the GP.\n\n\nobserved_summary_statistic - Summary statistic of the observed data. In our case this is zero\n\n\nSee abc_test.jl for usage example\n\n\nsource\n\n\n#\n\n\nGpAbc.scaled_squared_distance\n \n \nMethod\n.\n\n\nscaled_squared_distance(log_ell::AbstractArray{Float64, 1},\n    x::AbstractArray{Float64, 2}, z::AbstractArray{Float64, 2})\n\n\n\n\nCompute the scaled squared distance between \nx\n and \nz\n:\n\n\n\n\n\nr_{ij} = \\sum_{k=1}^d\\frac{(x_{ik}-z_{jk})^2}{l_k^2}\n\n\n\n\n\nArguments\n\n\n\n\nx, z\n: Input data, reshaped into 2-d arrays. \nx\n must have dimensions $n \\times d$; \nz\n must have dimensions $m \\times d$.\n\n\nlog_ell\n: logarithm of length scale(s). Can either be an array of size one (isotropic), or an array of size \nd\n (ARD)\n\n\n\n\nReturn\n\n\nAn $n \\times m$ matrix of scaled squared distances\n\n\nsource\n\n\n#\n\n\nGpAbc.scaled_squared_distance_grad\n \n \nMethod\n.\n\n\nscaled_squared_distance_grad(log_ell::AbstractArray{Float64, 1},\n    x::AbstractArray{Float64, 2}, z::AbstractArray{Float64, 2}, R::AbstractArray{Float64, 2})\n\n\n\n\nReturn the gradient of the \nscaled_squared_distance\n function with respect to logarigthms of length scales, based on the provided direction matrix.\n\n\nArguments\n\n\n\n\nx, z\n: Input data, reshaped into 2-d arrays. \nx\n must have dimensions $n \\times d$; \nz\n must have dimensions $m \\times d$.\n\n\nlog_ell\n: logarithm of length scale(s). Can either be an array of size one (isotropic), or an array of size \nd\n (ARD)\n\n\nR\n the direction matrix, $n \\times m$. This can be used to compute the gradient of a function that depends on \nscaled_squared_distance\n via the chain rule.\n\n\n\n\nReturn\n\n\nA vector of size \nlength(log_ell)\n, whose $k$'th element is equal to\n\n\n\n\n\n\\text{tr}(R \\frac{\\partial K}{\\partial l_k})\n\n\n\n\n\nsource\n\n\n#\n\n\nGpAbc.set_hyperparameters\n \n \nMethod\n.\n\n\nset_hyperparameters(gpm::GPModel, hypers::AbstractArray{Float64, 1})\n\n\n\n\nSet the hyperparameters of the \nGPModel\n\n\nsource", 
            "title": "Home"
        }
    ]
}