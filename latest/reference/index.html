<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · GpAbc.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>GpAbc.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../examples/">Examples</a></li><li class="current"><a class="toctext" href>Reference</a><ul class="internal"></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Reference</a></li></ul><a class="edit-page" href="https://github.com/tanhevg/GpAbc.jl/blob/master/docs/src/reference.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Reference</span><a class="fa fa-bars" href="#"></a></div></header><h3><a class="nav-anchor" id="Index-1" href="#Index-1">Index</a></h3><ul><li><a href="#GpAbc.AbstractGPKernel"><code>GpAbc.AbstractGPKernel</code></a></li><li><a href="#GpAbc.GPModel"><code>GpAbc.GPModel</code></a></li><li><a href="#GpAbc.GPModel-Tuple{}"><code>GpAbc.GPModel</code></a></li><li><a href="#GpAbc.GPModel"><code>GpAbc.GPModel</code></a></li><li><a href="#GpAbc.GPModel"><code>GpAbc.GPModel</code></a></li><li><a href="#GpAbc.MaternArdKernel"><code>GpAbc.MaternArdKernel</code></a></li><li><a href="#GpAbc.MaternIsoKernel"><code>GpAbc.MaternIsoKernel</code></a></li><li><a href="#GpAbc.SquaredExponentialArdKernel"><code>GpAbc.SquaredExponentialArdKernel</code></a></li><li><a href="#GpAbc.SquaredExponentialIsoKernel"><code>GpAbc.SquaredExponentialIsoKernel</code></a></li><li><a href="#GpAbc.ExponentialArdKernel-Tuple{}"><code>GpAbc.ExponentialArdKernel</code></a></li><li><a href="#GpAbc.ExponentialIsoKernel-Tuple{}"><code>GpAbc.ExponentialIsoKernel</code></a></li><li><a href="#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpAbc.covariance</code></a></li><li><a href="#GpAbc.covariance_diagonal-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>GpAbc.covariance_diagonal</code></a></li><li><a href="#GpAbc.covariance_grad-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpAbc.covariance_grad</code></a></li><li><a href="#GpAbc.covariance_training-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>GpAbc.covariance_training</code></a></li><li><a href="#GpAbc.get_hyperparameters_size-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,2}}"><code>GpAbc.get_hyperparameters_size</code></a></li><li><a href="#GpAbc.gp_loglikelihood-Tuple{GpAbc.GPModel}"><code>GpAbc.gp_loglikelihood</code></a></li><li><a href="#GpAbc.gp_loglikelihood-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}"><code>GpAbc.gp_loglikelihood</code></a></li><li><a href="#GpAbc.gp_loglikelihood_grad-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}"><code>GpAbc.gp_loglikelihood_grad</code></a></li><li><a href="#GpAbc.gp_loglikelihood_log-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}"><code>GpAbc.gp_loglikelihood_log</code></a></li><li><a href="#GpAbc.gp_regression-Tuple{Union{AbstractArray{Float64,1}, AbstractArray{Float64,2}},GpAbc.GPModel}"><code>GpAbc.gp_regression</code></a></li><li><a href="#GpAbc.gp_regression-Tuple{GpAbc.GPModel}"><code>GpAbc.gp_regression</code></a></li><li><a href="#GpAbc.gp_train-Tuple{GpAbc.GPModel}"><code>GpAbc.gp_train</code></a></li><li><a href="#GpAbc.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpAbc.scaled_squared_distance</code></a></li><li><a href="#GpAbc.scaled_squared_distance_grad-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpAbc.scaled_squared_distance_grad</code></a></li><li><a href="#GpAbc.set_hyperparameters-Tuple{GpAbc.GPModel,AbstractArray{Float64,1}}"><code>GpAbc.set_hyperparameters</code></a></li></ul><h3><a class="nav-anchor" id="GpAbc-1" href="#GpAbc-1">GpAbc</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.AbstractGPKernel" href="#GpAbc.AbstractGPKernel"><code>GpAbc.AbstractGPKernel</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">AbstractGPKernel</code></pre><p>Abstract kernel type. User-defined kernels should derive from it.</p><p>Implementations have to provide methods for <a href="#GpAbc.get_hyperparameters_size-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,2}}"><code>get_hyperparameters_size</code></a> and <a href="#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a>. Methods for <a href="#GpAbc.covariance_training-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>covariance_training</code></a>, <a href="#GpAbc.covariance_diagonal-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>covariance_diagonal</code></a> and <a href="#GpAbc.covariance_grad-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance_grad</code></a> are optional.</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/abstract_kernel.jl#L1-L9">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.GPModel" href="#GpAbc.GPModel"><code>GpAbc.GPModel</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">GPModel</code></pre><p>The main type that is used by most functions withing the package.</p><p>All data matrices are row-major.</p><p><strong>Fields</strong></p><ul><li><p><code>kernel::AbstractGPKernel</code>: the kernel</p></li><li><p><code>gp_training_x::AbstractArray{Float64, 2}</code>: training <code>x</code>. Size: <span>$n \times d$</span>.</p></li><li><p><code>gp_training_y::AbstractArray{Float64, 2}</code>: training <code>y</code>. Size: <span>$n \times 1$</span>.</p></li><li><p><code>gp_test_x::AbstractArray{Float64, 2}</code>: test <code>x</code>.  Size: <span>$m \times d$</span>.</p></li><li><p><code>gp_hyperparameters::AbstractArray{Float64, 1}</code>: kernel hyperparameters, followed by standard deviation of intrinsic noise <span>$\sigma_n$</span>, which is always the last element in the array.</p></li><li><p><code>cache::HPOptimisationCache</code>: cache of matrices that can be re-used between calls to <code>gp_loglikelihood</code> and <code>gp_loglikelihood_grad</code></p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/gp.jl#L22-L38">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.GPModel" href="#GpAbc.GPModel"><code>GpAbc.GPModel</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">GPModel(training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},
        training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},
        kernel::AbstractGPKernel
        [,test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0)])</code></pre><p>Constructor of <a href="#GpAbc.GPModel"><code>GPModel</code></a> that allows the kernel to be specified. Arguments that are passed as 1-d vectors will be reshaped into 2-d.</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/gp.jl#L65-L73">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.GPModel" href="#GpAbc.GPModel"><code>GpAbc.GPModel</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">GPModel(training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},
        training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}
        [,test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0)])</code></pre><p>Default constructor of <a href="#GpAbc.GPModel"><code>GPModel</code></a>, that will use <a href="#GpAbc.SquaredExponentialIsoKernel"><code>SquaredExponentialIsoKernel</code></a>. Arguments that are passed as 1-d vectors will be reshaped into 2-d.</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/gp.jl#L51-L58">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.GPModel-Tuple{}" href="#GpAbc.GPModel-Tuple{}"><code>GpAbc.GPModel</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">GPModel(;training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),
    training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),
    test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),
    kernel::AbstractGPKernel=SquaredExponentialIsoKernel(),
    gp_hyperparameters::AbstractArray{Float64, 1}=Array{Float64}(0))</code></pre><p>Constructor of <a href="#GpAbc.GPModel"><code>GPModel</code></a> with explicit arguments. Arguments that are passed as 1-d vectors will be reshaped into 2-d.</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/gp.jl#L81-L90">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.MaternArdKernel" href="#GpAbc.MaternArdKernel"><code>GpAbc.MaternArdKernel</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">MaternArdKernel &lt;: AbstractGPKernel</code></pre><p>Matérn kernel with distinct length scale for each dimention, <span>$l_k$</span>. Parameter <span>$\nu$</span> (nu) is passed in constructor. Currently, only values of <span>$\nu=1$</span>, <span>$\nu=3$</span> and <span>$\nu=5$</span> are supported.</p><div>\[\begin{aligned}
K_{\nu=1}(r) &amp;= \sigma_f^2e^{-\sqrt{r}}\\
K_{\nu=3}(r) &amp;= \sigma_f^2(1 + \sqrt{3r})e^{-\sqrt{3r}}\\
K_{\nu=5}(r) &amp;= \sigma_f^2(1 + \sqrt{3r} + \frac{5}{3}r)e^{-\sqrt{5r}}\\
r_{ij} &amp;= \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l_k^2}
\end{aligned}\]</div><p><span>$r_{ij}$</span> are computed by <a href="#GpAbc.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a></p><p><strong>Hyperparameters</strong></p><p>The length of hyperparameters array for this kernel depends on the dimensionality of the data. Assuming each data point is a vector in a <span>$d$</span>-dimensional space, this kernel needs <span>$d+1$</span> hyperparameters, in the following order:</p><ol><li><p><span>$\sigma_f$</span>: the signal standard deviation</p></li><li><p><span>$l_1, \ldots, l_d$</span>: the length scales for each dimension</p></li></ol></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/matern_kernels.jl#L54-L78">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.MaternIsoKernel" href="#GpAbc.MaternIsoKernel"><code>GpAbc.MaternIsoKernel</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">MaternIsoKernel &lt;: AbstractGPKernel</code></pre><p>Matérn kernel with uniform length scale across all dimensions, <span>$l$</span>. Parameter <span>$\nu$</span> (nu) is passed in constructor. Currently, only values of <span>$\nu=1$</span>, <span>$\nu=3$</span> and <span>$\nu=5$</span> are supported.</p><div>\[\begin{aligned}
K_{\nu=1}(r) &amp;= \sigma_f^2e^{-\sqrt{r}}\\
K_{\nu=3}(r) &amp;= \sigma_f^2(1 + \sqrt{3r})e^{-\sqrt{3r}}\\
K_{\nu=5}(r) &amp;= \sigma_f^2(1 + \sqrt{3r} + \frac{5}{3}r)e^{-\sqrt{5r}}\\
r_{ij} &amp;= \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l^2}
\end{aligned}\]</div><p><span>$r_{ij}$</span> are computed by <a href="#GpAbc.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a></p><p><strong>Hyperparameters</strong></p><p>Hyperparameters vector for this kernel must contain two elements, in the following order:</p><ol><li><p><span>$\sigma_f$</span>: the signal standard deviation</p></li><li><p><span>$l$</span>: the length scale</p></li></ol></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/matern_kernels.jl#L8-L30">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.SquaredExponentialArdKernel" href="#GpAbc.SquaredExponentialArdKernel"><code>GpAbc.SquaredExponentialArdKernel</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">SquaredExponentialArdKernel &lt;: AbstractGPKernel</code></pre><p>Squared exponential kernel with distinct length scale for each dimention, <span>$l_k$</span>.</p><div>\[\begin{aligned}
K(r) &amp; = \sigma_f^2 e^{-r/2} \\
r_{ij} &amp; = \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l_k^2}
\end{aligned}\]</div><p><span>$r_{ij}$</span> are computed by <a href="#GpAbc.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a></p><p><strong>Hyperparameters</strong></p><p>The length of hyperparameters array for this kernel depends on the dimensionality of the data. Assuming each data point is a vector in a <span>$d$</span>-dimensional space, this kernel needs <span>$d+1$</span> hyperparameters, in the following order:</p><ol><li><p><span>$\sigma_f$</span>: the signal standard deviation</p></li><li><p><span>$l_1, \ldots, l_d$</span>: the length scales for each dimension</p></li></ol></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/rbf_kernels.jl#L33-L52">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.SquaredExponentialIsoKernel" href="#GpAbc.SquaredExponentialIsoKernel"><code>GpAbc.SquaredExponentialIsoKernel</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">SquaredExponentialIsoKernel &lt;: AbstractGPKernel</code></pre><p>Squared exponential kernel with uniform length scale across all dimensions, <span>$l$</span>.</p><div>\[\begin{aligned}
K(r) &amp; = \sigma_f^2 e^{-r/2} \\
r_{ij} &amp; = \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l^2}
\end{aligned}\]</div><p><span>$r_{ij}$</span> are computed by <a href="#GpAbc.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a></p><p><strong>Hyperparameters</strong></p><p>Hyperparameters vector for this kernel must contain two elements, in the following order:</p><ol><li><p><span>$\sigma_f$</span>: the signal standard deviation</p></li><li><p><span>$l$</span>: the length scale</p></li></ol></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/rbf_kernels.jl#L7-L24">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.ExponentialArdKernel-Tuple{}" href="#GpAbc.ExponentialArdKernel-Tuple{}"><code>GpAbc.ExponentialArdKernel</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">ExponentialArdKernel</code></pre><p>Alias for <a href="#GpAbc.MaternArdKernel"><code>MaternArdKernel</code></a>(1)</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/matern_kernels.jl#L84-L88">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.ExponentialIsoKernel-Tuple{}" href="#GpAbc.ExponentialIsoKernel-Tuple{}"><code>GpAbc.ExponentialIsoKernel</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">ExponentialIsoKernel</code></pre><p>Alias for <a href="#GpAbc.MaternIsoKernel"><code>MaternIsoKernel</code></a>(1)</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/matern_kernels.jl#L47-L51">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}" href="#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpAbc.covariance</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">covariance(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, z::AbstractArray{Float64, 2})</code></pre><p>Return the covariance matrix. Should be overridden by kernel implementations.</p><p><strong>Arguments</strong></p><ul><li><p><code>ker</code>: The kernel object. Implementations must override with their own subtype.</p></li><li><p><code>log_theta</code>: natural logarithm of hyperparameters.</p></li><li><p><code>x, z</code>: Input data, reshaped into 2-d arrays. <code>x</code> must have dimensions <span>$n \times d$</span>; <code>z</code> must have dimensions <span>$m \times d$</span>.</p></li></ul><p><strong>Return</strong></p><p>The covariance matrix, of size <span>$n \times m$</span>.</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/abstract_kernel.jl#L12-L26">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.covariance_diagonal-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}" href="#GpAbc.covariance_diagonal-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>GpAbc.covariance_diagonal</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">covariance_diagonal(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2})</code></pre><p>This is a speedup version of <a href="#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a>, which is invoked if the caller is not interested in the entire covariance matrix, but only needs the variance, i.e. the diagonal of the covariance matrix.</p><p>Default method just returns <code>diag(covariance(...))</code>, with <code>x === z</code>. Kernel implementations can optionally override it to achieve betrer performance, by not computing the non diagonal elements of covariance matrix.</p><p>See <a href="#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a> for description of arguments.</p><p><strong>Return</strong></p><p>The 1-d array of variances, of size <code>size(x, 1)</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/abstract_kernel.jl#L49-L65">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.covariance_grad-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}" href="#GpAbc.covariance_grad-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpAbc.covariance_grad</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">covariance_grad(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, R::AbstractArray{Float64, 2})</code></pre><p>Return the gradient of the covariance function with respect to logarigthms of hyperparameters, based on the provided direction matrix.</p><p>This function can be optionally overridden by kernel implementations. If the gradient function is not provided, <a href="#GpAbc.gp_train-Tuple{GpAbc.GPModel}"><code>gp_train</code></a> will fail back to <code>NelderMead</code> algorithm by default.</p><p><strong>Arguments</strong></p><ul><li><p><code>ker</code>: The kernel object. Implementations must override with their own subtype.</p></li><li><p><code>log_theta</code>:  natural logarithm of hyperparameters</p></li><li><p><code>x</code>: Training data, reshaped into a 2-d array. <code>x</code> must have dimensions <span>$n \times d$</span>.</p></li><li><p><code>R</code> the directional matrix, <span>$n \times n$</span></p></li></ul><div>\[R = \frac{1}{\sigma_n^2}(\alpha * \alpha^T - K^{-1}); \alpha = K^{-1}y\]</div><p><strong>Return</strong></p><p>A vector of size <code>length(log_theta)</code>, whose <span>$j$</span>&#39;th element is equal to</p><div>\[tr(R \frac{\partial K}{\partial \eta_j})\]</div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/abstract_kernel.jl#L71-L97">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.covariance_training-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}" href="#GpAbc.covariance_training-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>GpAbc.covariance_training</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">covariance_training(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},
    training_x::AbstractArray{Float64, 2})</code></pre><p>This is a speedup version of <a href="#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a>, which is only called during traing sequence. Intermediate matrices computed in this function for particular hyperparameters can be cached and reused subsequently, either in this function or in <a href="#GpAbc.covariance_grad-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance_grad</code></a></p><p>Default method just delegates to <a href="#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a> with <code>x === z</code>. Kernel implementations can optionally override it for betrer performance.</p><p>See <a href="#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a> for description of arguments and return values.</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/abstract_kernel.jl#L30-L43">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.get_hyperparameters_size-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,2}}" href="#GpAbc.get_hyperparameters_size-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,2}}"><code>GpAbc.get_hyperparameters_size</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">get_hyperparameters_size(kernel::AbstractGPKernel, training_data::AbstractArray{Float64, 2})</code></pre><p>Return the number of hyperparameters for used by this kernel on this training data set. Should be overridden by kernel implementations.</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/abstract_kernel.jl#L103-L108">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.gp_loglikelihood-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}" href="#GpAbc.gp_loglikelihood-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}"><code>GpAbc.gp_loglikelihood</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">gp_loglikelihood(theta::AbstractArray{Float64, 1}, gpm::GPModel)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/gp.jl#L177-L179">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.gp_loglikelihood-Tuple{GpAbc.GPModel}" href="#GpAbc.gp_loglikelihood-Tuple{GpAbc.GPModel}"><code>GpAbc.gp_loglikelihood</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">gp_loglikelihood(gpm::GPModel)</code></pre><p>Compute the log likelihood function, based on the kernel and training data specified in <code>gpm</code>.</p><div>\[log p(y \vert X, \theta) = - \frac{1}{2}(y^TK^{-1}y + log \vert K \vert + n log 2 \pi)\]</div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/gp.jl#L167-L174">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.gp_loglikelihood_grad-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}" href="#GpAbc.gp_loglikelihood_grad-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}"><code>GpAbc.gp_loglikelihood_grad</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">gp_loglikelihood_grad(theta::AbstractArray{Float64, 1}, gpem::GPModel)</code></pre><p>Gradient of the log likelihood function (<a href="#GpAbc.gp_loglikelihood_log-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}"><code>gp_loglikelihood_log</code></a>) with respect to logged hyperparameters.</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/gp.jl#L182-L187">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.gp_loglikelihood_log-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}" href="#GpAbc.gp_loglikelihood_log-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}"><code>GpAbc.gp_loglikelihood_log</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">gp_loglikelihood_log(theta::AbstractArray{Float64, 1}, gpm::GPModel)</code></pre><p>Log likelihood function with log hyperparameters. This is the target function of the hyperparameters optimisation procedure. Its gradient is coputed by <a href="#GpAbc.gp_loglikelihood_grad-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}"><code>gp_loglikelihood_grad</code></a>.</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/gp.jl#L150-L155">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.gp_regression-Tuple{GpAbc.GPModel}" href="#GpAbc.gp_regression-Tuple{GpAbc.GPModel}"><code>GpAbc.gp_regression</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">gp_regression(gpm::GPModel; &lt;optional keyword arguments&gt;)</code></pre><p>Run the Gaussian Process Regression.</p><p><strong>Arguments</strong></p><ul><li><p><code>gpm</code>: the <a href="#GpAbc.GPModel"><code>GPModel</code></a>, that contains the training data (x and y), the kernel, the hyperparameters and the test data for running the regression.</p></li><li><p><code>test_x</code>: if specified, overrides the test x in <code>gpm</code>. Size <span>$m \times d$</span>.</p></li><li><p><code>log_level::Int</code> (optional): log level. Default is <code>0</code>, which is no logging at all. <code>1</code> makes <code>gp_regression</code> print basic information to standard output.</p></li><li><p><code>full_covariance_matrix::Bool</code> (optional): whether we need the full covariance matrix, or just the variance vector. Defaults to <code>false</code> (i.e. just the variance).</p></li><li><p><code>batch_size::Int</code> (optional): If <code>full_covariance_matrix</code> is set to <code>false</code>, then the mean and variance vectors will be computed in batches of this size, to avoid allocating huge matrices. Defaults to 1000.</p></li><li><p><code>observation_noise::Bool</code> (optional): whether the observation noise (with variance <span>$\sigma_n^2$</span>) should be included in the output variance. Defaults to <code>true</code>.</p></li></ul><p><strong>Return</strong></p><p>A tuple of <code>(mean, var)</code>. <code>mean</code> is a mean vector of the output multivariate Normal distribution, of size <span>$m$</span>. <code>var</code> is either the covariance matrix of size <span>$m \times m$</span>, or a variance vector of size <span>$m$</span>, depending on <code>full_covariance_matrix</code> flag.</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/gp.jl#L204-L227">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.gp_regression-Tuple{Union{AbstractArray{Float64,1}, AbstractArray{Float64,2}},GpAbc.GPModel}" href="#GpAbc.gp_regression-Tuple{Union{AbstractArray{Float64,1}, AbstractArray{Float64,2}},GpAbc.GPModel}"><code>GpAbc.gp_regression</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">gp_regression(test_x::Union{AbstractArray{Float64, 1}, AbstractArray{Float64, 2}},
    gpem::GPModel; &lt;optional keyword arguments&gt;)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/gp.jl#L232-L235">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.gp_train-Tuple{GpAbc.GPModel}" href="#GpAbc.gp_train-Tuple{GpAbc.GPModel}"><code>GpAbc.gp_train</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">gp_train(gpm::GPModel; &lt;optional keyword arguments&gt;)</code></pre><p>Find Maximum Likelihood Estimate of Gaussian Process hyperparameters by maximising <a href="#GpAbc.gp_loglikelihood-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}"><code>gp_loglikelihood</code></a>, using <a href="http://julianlsolvers.github.io/Optim.jl/stable/"><code>Optim</code></a> package.</p><p><strong>Arguments</strong></p><ul><li><p><code>gpm</code>: the <a href="#GpAbc.GPModel"><code>GPModel</code></a>, that contains the training data (x and y), the kernel and the starting hyperparameters that will be used for optimisation.</p></li><li><p><code>optimisation_solver_type::Type{&lt;:Optim.Optimizer}</code> (optional): the solver to use. If not given, then <code>ConjugateGradient</code> will be used for kernels that have gradient implementation, and <code>NelderMead</code> will be used for those that don&#39;t.</p></li><li><p><code>hp_lower::AbstractArray{Float64, 1}</code> (optional): the lower boundary for box optimisation. Defaults to <span>$e^{-10}$</span> for all hyperparameters.</p></li><li><p><code>hp_upper::AbstractArray{Float64, 1}</code> (optional): the upper boundary for box optimisation. Defaults to <span>$e^{10}$</span> for all hyperparameters.</p></li><li><p><code>log_level::Int</code> (optional): log level. Default is <code>0</code>, which is no logging at all. <code>1</code> makes <code>gp_train</code> print basic information to standard output. <code>2</code> switches <code>Optim</code> logging on, in addition to <code>1</code>.</p></li></ul><p><strong>Return</strong></p><p>The list of all hyperparameters, including the standard deviation of the measurement noise <span>$\sigma_n$</span>. Note that after this function returns, the hyperparameters of <code>gpm</code> will be set to the optimised value, and there is no need to call <a href="#GpAbc.set_hyperparameters-Tuple{GpAbc.GPModel,AbstractArray{Float64,1}}"><code>set_hyperparameters</code></a> once again.</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/gp_optimisation.jl#L17-L42">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}" href="#GpAbc.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpAbc.scaled_squared_distance</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">scaled_squared_distance(log_ell::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, z::AbstractArray{Float64, 2})</code></pre><p>Compute the scaled squared distance between <code>x</code> and <code>z</code>:</p><div>\[r_{ij} = \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l_k^2}\]</div><p>The gradient of this function with respect to length scale hyperparameter(s) is returned by <a href="#GpAbc.scaled_squared_distance_grad-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance_grad</code></a>.</p><p><strong>Arguments</strong></p><ul><li><p><code>x, z</code>: Input data, reshaped into 2-d arrays. <code>x</code> must have dimensions <span>$n \times d$</span>; <code>z</code> must have dimensions <span>$m \times d$</span>.</p></li><li><p><code>log_ell</code>: logarithm of length scale(s). Can either be an array of size one (isotropic), or an array of size <code>d</code> (ARD)</p></li></ul><p><strong>Return</strong></p><p>An <span>$n \times m$</span> matrix of scaled squared distances</p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/scaled_squared_distance.jl#L1-L20">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.scaled_squared_distance_grad-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2},AbstractArray{Float64,2}}" href="#GpAbc.scaled_squared_distance_grad-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>GpAbc.scaled_squared_distance_grad</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">scaled_squared_distance_grad(log_ell::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, z::AbstractArray{Float64, 2}, R::AbstractArray{Float64, 2})</code></pre><p>Return the gradient of the <a href="#GpAbc.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a> function with respect to logarigthms of length scales, based on the provided direction matrix.</p><p><strong>Arguments</strong></p><ul><li><p><code>x, z</code>: Input data, reshaped into 2-d arrays. <code>x</code> must have dimensions <span>$n \times d$</span>; <code>z</code> must have dimensions <span>$m \times d$</span>.</p></li><li><p><code>log_ell</code>: logarithm of length scale(s). Can either be an array of size one (isotropic), or an array of size <code>d</code> (ARD)</p></li><li><p><code>R</code> the direction matrix, <span>$n \times m$</span>. This can be used to compute the gradient of a function that depends on <a href="#GpAbc.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a> via the chain rule.</p></li></ul><p><strong>Return</strong></p><p>A vector of size <code>length(log_ell)</code>, whose <span>$k$</span>&#39;th element is equal to</p><div>\[\text{tr}(R \frac{\partial K}{\partial l_k})\]</div></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/kernels/scaled_squared_distance.jl#L45-L65">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="GpAbc.set_hyperparameters-Tuple{GpAbc.GPModel,AbstractArray{Float64,1}}" href="#GpAbc.set_hyperparameters-Tuple{GpAbc.GPModel,AbstractArray{Float64,1}}"><code>GpAbc.set_hyperparameters</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">set_hyperparameters(gpm::GPModel, hypers::AbstractArray{Float64, 1})</code></pre><p>Set the hyperparameters of the <a href="#GpAbc.GPModel"><code>GPModel</code></a></p></div><a class="source-link" target="_blank" href="https://github.com/tanhevg/GpAbc.jl/blob/4ba0afdee0df7e3f36b6f16c37356f1f7ec4df7c/src/gp/gp.jl#L135-L139">source</a></section><footer><hr/><a class="previous" href="../examples/"><span class="direction">Previous</span><span class="title">Examples</span></a></footer></article></body></html>
