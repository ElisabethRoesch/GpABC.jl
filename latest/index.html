<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · GpABC.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>GpABC.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Home</a><ul class="internal"><li><a class="toctext" href="#Notation-1">Notation</a></li><li><a class="toctext" href="#Basic-Usage-1">Basic Usage</a></li><li><a class="toctext" href="#Training-the-GP-1">Training the GP</a></li><li><a class="toctext" href="#Kernels-1">Kernels</a></li></ul></li><li><a class="toctext" href="examples/">Examples</a></li><li><a class="toctext" href="reference/">Reference</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Home</a></li></ul><a class="edit-page" href="https://github.com/tanhevg/GpABC.jl/blob/master/docs/src/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Home</span><a class="fa fa-bars" href="#"></a></div></header><ul><li><p><a href="#Notation-1">Notation</a></p></li><li><p><a href="#Basic-Usage-1">Basic Usage</a></p></li><li><p><a href="#Training-the-GP-1">Training the GP</a></p></li><li><p><a href="#Kernels-1">Kernels</a></p></li></ul><p><strong>TODO markdown does not support include, so copy-paste content from /README.md once it is finalised</strong></p><h2><a class="nav-anchor" id="Notation-1" href="#Notation-1">Notation</a></h2><p>Throughout this manual, we denote the number of training points as <span>$n$</span>, and the number of test points as <span>$m$</span>. The number of dimensions is denoted as <span>$d$</span>. For one-dimensional case, where each individual training and test point is just a real number, both one-dimensional and two-dimensional arrays are accepted as inputs. In <a href="examples/#example-1-1">Basic Gaussian Process Regression Example</a> <code>training_x</code> can either be a vector of size <span>$n$</span>, or an <span>$n \times 1$</span> matrix. For a multidimensional case, where test and training points are elements of a <span>$d$</span>-dimentional space, all inputs have to be row major, so <code>training_x</code> and <code>test_x</code> become an <span>$n \times d$</span> and an <span>$m \times d$</span> matrices, respectively.</p><h2><a class="nav-anchor" id="Basic-Usage-1" href="#Basic-Usage-1">Basic Usage</a></h2><p>The package is built around a type <a href="reference/#GpABC.GPModel"><code>GPModel</code></a>, which encapsulates all the information required for training the Gaussian Process and performing the regression. In the simplest scenario the user would instantiate this type with some training data and labels, provide the hyperparameters and run the regression. By default, <a href="reference/#GpABC.SquaredExponentialIsoKernel"><code>SquaredExponentialIsoKernel</code></a> will be used. This scenario is illustrated by <a href="examples/#example-1-1">Basic Gaussian Process Regression Example</a>.</p><h2><a class="nav-anchor" id="Training-the-GP-1" href="#Training-the-GP-1">Training the GP</a></h2><p>Normally, kernel hyperparameters are not known in advance. In this scenario the training function <a href="reference/#GpABC.gp_train-Tuple{GpABC.GPModel}"><code>gp_train</code></a> should be used to find the Maximum Likelihood Estimate (MLE) of hyperparameters. This is demonstrated in <a href="examples/#example-2-1">Optimising Hyperparameters for GP Regression Example</a>.</p><p>GaussProABC uses <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim</a> package for optimising the hyperparameters. By default, <a href="http://julianlsolvers.github.io/Optim.jl/stable/algo/cg/">Conjugate Gradient</a> bounded box optimisation is used, as long as the gradient with respect to hyperparameters is implemented for the kernel function. If the gradient implementation is not provided, <a href="http://julianlsolvers.github.io/Optim.jl/stable/algo/nelder_mead/">Nelder Mead</a> optimiser is used by default.</p><p>The starting point of the optimisation can be specified by calling <a href="reference/#GpABC.set_hyperparameters-Tuple{GpABC.GPModel,AbstractArray{Float64,1}}"><code>set_hyperparameters</code></a>. If the starting point has not been provided, optimisation will start from all hyperparameters set to 1. Default upper and lower bounds are set to <span>$e^{10}$</span> and <span>$e^{−10}$</span> , respectively, for each hyperparameter.</p><p>For numerical stability the package uses logarithms of hyperparameters internally, when calling the log likelihood and kernel functions. Logarithmisation and exponentiation back takes place in <a href="reference/#GpABC.gp_train-Tuple{GpABC.GPModel}"><code>gp_train</code></a> function.</p><p>The log likelihood function with log hyperparameters is implemented by <a href="reference/#GpABC.gp_loglikelihood_log-Tuple{AbstractArray{Float64,1},GpABC.GPModel}"><code>gp_loglikelihood_log</code></a>. This is the target function of the optimisation procedure in <a href="reference/#GpABC.gp_train-Tuple{GpABC.GPModel}"><code>gp_train</code></a>. There is also a version of log likelihood with actual (non-log) hyperparameters: <a href="reference/#GpABC.gp_loglikelihood-Tuple{AbstractArray{Float64,1},GpABC.GPModel}"><code>gp_loglikelihood</code></a>. The gradient of the log likelihood function with respect to logged hyperparameters is implemented by <a href="reference/#GpABC.gp_loglikelihood_grad-Tuple{AbstractArray{Float64,1},GpABC.GPModel}"><code>gp_loglikelihood_grad</code></a>.</p><p>Depending on the kernel, it is not uncommon for the log likelihood function to have multiple local optima. If a trained GP produces an unsatisfactory data fit, one possible workaround is trying to run <a href="reference/#GpABC.gp_train-Tuple{GpABC.GPModel}"><code>gp_train</code></a> several times with random starting points. This approach is demonstrated in <a href="examples/#example-3-1">Advanced Usage of gp_train example</a>.</p><p><code>Optim</code> has a built in constraint of running no more than 1000 iterations of any optimisation algorithm. <code>GpABC</code> relies on this feature to ensure that the training procedure does not get stuck forever. As a consequence, the optimizer might exit prematurely, before reaching the local optimum. Setting <code>log_level</code> argument of <a href="reference/#GpABC.gp_train-Tuple{GpABC.GPModel}"><code>gp_train</code></a> to a value greater than zero will make it log its actions to standard output, including whether the local minimum has been reached or not.</p><h2><a class="nav-anchor" id="Kernels-1" href="#Kernels-1">Kernels</a></h2><p><code>GpABC</code> ships with an extensible library of kernel functions. Each kernel is represented with a type that derives from <a href="reference/#GpABC.AbstractGPKernel"><code>AbstractGPKernel</code></a>:</p><ul><li><p><a href="reference/#GpABC.SquaredExponentialIsoKernel"><code>SquaredExponentialIsoKernel</code></a></p></li><li><p><a href="reference/#GpABC.SquaredExponentialArdKernel"><code>SquaredExponentialArdKernel</code></a></p></li><li><p><a href="reference/#GpABC.MaternIsoKernel"><code>MaternIsoKernel</code></a></p></li><li><p><a href="reference/#GpABC.MaternArdKernel"><code>MaternArdKernel</code></a></p></li><li><p><a href="reference/#GpABC.ExponentialIsoKernel-Tuple{}"><code>ExponentialIsoKernel</code></a></p></li><li><p><a href="reference/#GpABC.ExponentialArdKernel-Tuple{}"><code>ExponentialArdKernel</code></a></p></li></ul><p>These kernels rely on matrix of scaled squared distances between training/test inputs <span>$[r_{ij}]$</span>, which is computed by <a href="reference/#GpABC.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a> function. The gradient vector of scaled squared distance derivatives with respect to length scale hyperparameter(s) is returned by <a href="reference/#GpABC.scaled_squared_distance_grad-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance_grad</code></a> function.</p><p>The kernel covariance matrix is returned by function <a href="reference/#GpABC.covariance-Tuple{GpABC.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a>. Optional speedups of this function <a href="reference/#GpABC.covariance_diagonal-Tuple{GpABC.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>covariance_diagonal</code></a> and <a href="reference/#GpABC.covariance_training-Tuple{GpABC.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>covariance_training</code></a> are implemented for the pre-shipped kernels. The gradient with respect to log hyperparameters is computed by <a href="reference/#GpABC.covariance_grad-Tuple{GpABC.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance_grad</code></a>. The <code>log_theta</code> argument refers to the logarithms of kernel hyperparameters. Note that hyperparameters that do not affect the kernel (e.g. <span>$\sigma_n$</span> ) are not included in <code>log_theta</code>.</p><p>Custom kernels functions can be implemented  by adding more types that inherit from <a href="reference/#GpABC.AbstractGPKernel"><code>AbstractGPKernel</code></a>. This is demonstrated in <a href="examples/#example-4-1">Using a Custom Kernel Example</a></p><footer><hr/><a class="next" href="examples/"><span class="direction">Next</span><span class="title">Examples</span></a></footer></article></body></html>
