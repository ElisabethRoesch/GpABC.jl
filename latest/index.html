<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Description...">
  <meta name="author" content="tanhevg">
  <link rel="shortcut icon" href="./img/favicon.ico">
  <title>Home - GpAbc.jl</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="./css/theme.css" type="text/css" />
  <link rel="stylesheet" href="./css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="./css/highlight.css">
  <link href="./assets/Documenter.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Home";
    var mkdocs_page_input_path = "index.md";
    var mkdocs_page_url = "/";
  </script>
  
  <script src="./js/jquery-2.1.1.min.js"></script>
  <script src="./js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="./js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="." class="icon icon-home"> GpAbc.jl</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1 current">
		
    <a class="current" href=".">Home</a>
    <ul class="subnav">
            
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href=".">GpAbc.jl</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".">Docs</a> &raquo;</li>
    
      
    
    <li>Home</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/tanhevg/GpAbc.jl/edit/master/docs/index.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <p><a id='GpAbc.AbstractGPKernel' href='#GpAbc.AbstractGPKernel'>#</a>
<strong><code>GpAbc.AbstractGPKernel</code></strong> &mdash; <em>Type</em>.</p>
<pre><code>AbstractGPKernel
</code></pre>

<p>Abstract kernel type. User-defined kernels should derive from it.</p>
<p>Implementations have to provide methods for <a href=".#GpAbc.get_hyperparameters_size-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,2}}"><code>get_hyperparameters_size</code></a> and <a href=".#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a>. Methods for <a href=".#GpAbc.covariance_training-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>covariance_training</code></a>, <a href=".#GpAbc.covariance_diagonal-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}"><code>covariance_diagonal</code></a> and <a href=".#GpAbc.covariance_grad-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance_grad</code></a> are optional.</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/abstract_kernel.jl#L1-L9' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.GPModel' href='#GpAbc.GPModel'>#</a>
<strong><code>GpAbc.GPModel</code></strong> &mdash; <em>Type</em>.</p>
<pre><code>GPModel
</code></pre>

<p>The main type that is used by most functions withing the package.</p>
<p>All data matrices are row-major.</p>
<p><strong>Fields</strong></p>
<ul>
<li><code>kernel::AbstractGPKernel</code>: the kernel</li>
<li><code>gp_training_x::AbstractArray{Float64, 2}</code>: training <code>x</code>. Size: $n \times d$.</li>
<li><code>gp_training_y::AbstractArray{Float64, 2}</code>: training <code>y</code>. Size: $n \times 1$.</li>
<li><code>gp_test_x::AbstractArray{Float64, 2}</code>: test <code>x</code>.  Size: $m \times d$.</li>
<li><code>gp_hyperparameters::AbstractArray{Float64, 1}</code>: kernel hyperparameters, followed by standard deviation of intrinsic noise $\sigma_n$, which is always the last element in the array.</li>
<li><code>cache::HPOptimisationCache</code>: cache of matrices that can be re-used between calls to <code>gp_loglikelihood</code> and <code>gp_loglikelihood_grad</code></li>
</ul>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/gp.jl#L22-L38' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.GPModel' href='#GpAbc.GPModel'>#</a>
<strong><code>GpAbc.GPModel</code></strong> &mdash; <em>Type</em>.</p>
<pre><code>GPModel(training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},
        training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},
        kernel::AbstractGPKernel
        [,test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0)])
</code></pre>

<p>Constructor of <a href=".#GpAbc.GPModel"><code>GPModel</code></a> that allows the kernel to be specified. Arguments that are passed as 1-d vectors will be reshaped into 2-d.</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/gp.jl#L65-L73' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.GPModel' href='#GpAbc.GPModel'>#</a>
<strong><code>GpAbc.GPModel</code></strong> &mdash; <em>Type</em>.</p>
<pre><code>GPModel(training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}},
        training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}
        [,test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0)])
</code></pre>

<p>Default constructor of <a href=".#GpAbc.GPModel"><code>GPModel</code></a>, that will use <a href=".#GpAbc.SquaredExponentialIsoKernel"><code>SquaredExponentialIsoKernel</code></a>. Arguments that are passed as 1-d vectors will be reshaped into 2-d.</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/gp.jl#L51-L58' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.GPModel-Tuple{}' href='#GpAbc.GPModel-Tuple{}'>#</a>
<strong><code>GpAbc.GPModel</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>GPModel(;training_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),
    training_y::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),
    test_x::Union{AbstractArray{Float64, 2}, AbstractArray{Float64, 1}}=zeros(0,0),
    kernel::AbstractGPKernel=SquaredExponentialIsoKernel(),
    gp_hyperparameters::AbstractArray{Float64, 1}=Array{Float64}(0))
</code></pre>

<p>Constructor of <a href=".#GpAbc.GPModel"><code>GPModel</code></a> with explicit arguments. Arguments that are passed as 1-d vectors will be reshaped into 2-d.</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/gp.jl#L81-L90' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.MaternArdKernel' href='#GpAbc.MaternArdKernel'>#</a>
<strong><code>GpAbc.MaternArdKernel</code></strong> &mdash; <em>Type</em>.</p>
<pre><code>MaternArdKernel &lt;: AbstractGPKernel
</code></pre>

<p>Matérn kernel with distinct length scale for each dimention, $l_k$. Parameter $\nu$ (nu) is passed in constructor. Currently, only values of $\nu=1$, $\nu=3$ and $\nu=5$ are supported.</p>
<p>
<script type="math/tex; mode=display">
\begin{aligned}
K_{\nu=1}(r) &= \sigma_f^2e^{-\sqrt{r}}\\
K_{\nu=3}(r) &= \sigma_f^2(1 + \sqrt{3r})e^{-\sqrt{3r}}\\
K_{\nu=5}(r) &= \sigma_f^2(1 + \sqrt{3r} + \frac{5}{3}r)e^{-\sqrt{5r}}\\
r_{ij} &= \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l_k^2}
\end{aligned}
</script>
</p>
<p><strong>Hyperparameters</strong></p>
<p>The length of hyperparameters array for this kernel depends on the dimensionality of the data. Assuming each data point is a vector in a $d$-dimensional space, this kernel needs $d+1$ hyperparameters, in the following order:</p>
<ol>
<li>$\sigma_f$: the signal standard deviation</li>
<li>$l_1, \ldots, l_d$: the length scales for each dimension</li>
</ol>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/matern_kernels.jl#L52-L74' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.MaternIsoKernel' href='#GpAbc.MaternIsoKernel'>#</a>
<strong><code>GpAbc.MaternIsoKernel</code></strong> &mdash; <em>Type</em>.</p>
<pre><code>MaternIsoKernel &lt;: AbstractGPKernel
</code></pre>

<p>Matérn kernel with uniform length scale across all dimensions, $l$. Parameter $\nu$ (nu) is passed in constructor. Currently, only values of $\nu=1$, $\nu=3$ and $\nu=5$ are supported.</p>
<p>
<script type="math/tex; mode=display">
\begin{aligned}
K_{\nu=1}(r) &= \sigma_f^2e^{-\sqrt{r}}\\
K_{\nu=3}(r) &= \sigma_f^2(1 + \sqrt{3r})e^{-\sqrt{3r}}\\
K_{\nu=5}(r) &= \sigma_f^2(1 + \sqrt{3r} + \frac{5}{3}r)e^{-\sqrt{5r}}\\
r_{ij} &= \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l^2}
\end{aligned}
</script>
</p>
<p><strong>Hyperparameters</strong></p>
<p>Hyperparameters vector for this kernel must contain two elements, in the following order:</p>
<ol>
<li>$\sigma_f$: the signal standard deviation</li>
<li>$l$: the length scale</li>
</ol>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/matern_kernels.jl#L8-L28' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.SquaredExponentialArdKernel' href='#GpAbc.SquaredExponentialArdKernel'>#</a>
<strong><code>GpAbc.SquaredExponentialArdKernel</code></strong> &mdash; <em>Type</em>.</p>
<pre><code>SquaredExponentialArdKernel &lt;: AbstractGPKernel
</code></pre>

<p>Squared exponential kernel with distinct length scale for each dimention, $l_k$.</p>
<p>
<script type="math/tex; mode=display">
K(r) = \sigma_f^2 e^{-r/2}; r_{ij} = \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l_k^2}
</script>
</p>
<p><strong>Hyperparameters</strong></p>
<p>The length of hyperparameters array for this kernel depends on the dimensionality of the data. Assuming each data point is a vector in a $d$-dimensional space, this kernel needs $d+1$ hyperparameters, in the following order:</p>
<ol>
<li>$\sigma_f$: the signal standard deviation</li>
<li>$l_1, \ldots, l_d$: the length scales for each dimension</li>
</ol>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/rbf_kernels.jl#L28-L42' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.SquaredExponentialIsoKernel' href='#GpAbc.SquaredExponentialIsoKernel'>#</a>
<strong><code>GpAbc.SquaredExponentialIsoKernel</code></strong> &mdash; <em>Type</em>.</p>
<pre><code>SquaredExponentialIsoKernel &lt;: AbstractGPKernel
</code></pre>

<p>Squared exponential kernel with uniform length scale across all dimensions, $l$.</p>
<p>
<script type="math/tex; mode=display">
K(r) = \sigma_f^2 e^{-r/2}; r_{ij} = \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l^2}
</script>
</p>
<p><strong>Hyperparameters</strong></p>
<p>Hyperparameters vector for this kernel must contain two elements, in the following order:</p>
<ol>
<li>$\sigma_f$: the signal standard deviation</li>
<li>$l$: the length scale</li>
</ol>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/rbf_kernels.jl#L7-L19' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.ExponentialArdKernel-Tuple{}' href='#GpAbc.ExponentialArdKernel-Tuple{}'>#</a>
<strong><code>GpAbc.ExponentialArdKernel</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>ExponentialArdKernel
</code></pre>

<p>Alias for <a href=".#GpAbc.MaternArdKernel"><code>MaternArdKernel</code></a>(1)</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/matern_kernels.jl#L80-L84' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.ExponentialIsoKernel-Tuple{}' href='#GpAbc.ExponentialIsoKernel-Tuple{}'>#</a>
<strong><code>GpAbc.ExponentialIsoKernel</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>ExponentialIsoKernel
</code></pre>

<p>Alias for <a href=".#GpAbc.MaternIsoKernel"><code>MaternIsoKernel</code></a>(1)</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/matern_kernels.jl#L45-L49' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}' href='#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}'>#</a>
<strong><code>GpAbc.covariance</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>covariance(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, z::AbstractArray{Float64, 2})
</code></pre>

<p>Return the covariance matrix. Should be overridden by kernel implementations.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>ker</code>: The kernel object. Implementations must override with their own subtype.</li>
<li><code>log_theta</code>: natural logarithm of hyperparameters.</li>
<li><code>x, z</code>: Input data, reshaped into 2-d arrays. <code>x</code> must have dimensions $n \times d$; <code>z</code> must have dimensions $m \times d$.</li>
</ul>
<p><strong>Return</strong></p>
<p>The covariance matrix, of size $n \times m$.</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/abstract_kernel.jl#L12-L26' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.covariance_diagonal-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}' href='#GpAbc.covariance_diagonal-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}'>#</a>
<strong><code>GpAbc.covariance_diagonal</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>covariance_diagonal(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2})
</code></pre>

<p>This is a speedup version of <a href=".#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a>, which is invoked if the caller is not interested in the entire covariance matrix, but only needs the variance, i.e. the diagonal of the covariance matrix.</p>
<p>Default method just returns <code>diag(covariance(...))</code>, with <code>x === z</code>. Kernel implementations can optionally override it to achieve betrer performance, by not computing the non diagonal elements of covariance matrix.</p>
<p>See <a href=".#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a> for description of arguments.</p>
<p><strong>Return</strong></p>
<p>The 1-d array of variances, of size <code>size(x, 1)</code>.</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/abstract_kernel.jl#L49-L65' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.covariance_grad-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}' href='#GpAbc.covariance_grad-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}'>#</a>
<strong><code>GpAbc.covariance_grad</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>covariance_grad(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, R::AbstractArray{Float64, 2})
</code></pre>

<p>Return the gradient of the covariance function with respect to logarigthms of hyperparameters, based on the provided direction matrix.</p>
<p>This function can be optionally overridden by kernel implementations. If the gradient function is not provided, <a href=".#GpAbc.gp_train-Tuple{GpAbc.GPModel}"><code>gp_train</code></a> will fail back to <code>NelderMead</code> algorithm by default.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>ker</code>: The kernel object. Implementations must override with their own subtype.</li>
<li><code>log_theta</code>:  natural logarithm of hyperparameters</li>
<li><code>x</code>: Training data, reshaped into a 2-d array. <code>x</code> must have dimensions $n \times d$.</li>
<li><code>R</code> the directional matrix, $n \times n$</li>
</ul>
<p>
<script type="math/tex; mode=display">
R = \frac{1}{\sigma_n^2}(\alpha * \alpha^T - K^{-1}); \alpha = K^{-1}y
</script>
</p>
<p><strong>Return</strong></p>
<p>A vector of size <code>length(log_theta)</code>, whose $j$'th element is equal to</p>
<p>
<script type="math/tex; mode=display">
tr(R \frac{\partial K}{\partial \eta_j})
</script>
</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/abstract_kernel.jl#L71-L97' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.covariance_training-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}' href='#GpAbc.covariance_training-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2}}'>#</a>
<strong><code>GpAbc.covariance_training</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>covariance_training(ker::AbstractGPKernel, log_theta::AbstractArray{Float64, 1},
    training_x::AbstractArray{Float64, 2})
</code></pre>

<p>This is a speedup version of <a href=".#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a>, which is only called during traing sequence. Intermediate matrices computed in this function for particular hyperparameters can be cached and reused subsequently, either in this function or in <a href=".#GpAbc.covariance_grad-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance_grad</code></a></p>
<p>Default method just delegates to <a href=".#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a> with <code>x === z</code>. Kernel implementations can optionally override it for betrer performance.</p>
<p>See <a href=".#GpAbc.covariance-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>covariance</code></a> for description of arguments and return values.</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/abstract_kernel.jl#L30-L43' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.get_hyperparameters_size-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,2}}' href='#GpAbc.get_hyperparameters_size-Tuple{GpAbc.AbstractGPKernel,AbstractArray{Float64,2}}'>#</a>
<strong><code>GpAbc.get_hyperparameters_size</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>get_hyperparameters_size(kernel::AbstractGPKernel, training_data::AbstractArray{Float64, 2})
</code></pre>

<p>Return the number of hyperparameters for used by this kernel on this training data set. Should be overridden by kernel implementations.</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/abstract_kernel.jl#L103-L108' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.gp_loglikelihood-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}' href='#GpAbc.gp_loglikelihood-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}'>#</a>
<strong><code>GpAbc.gp_loglikelihood</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>gp_loglikelihood(theta::AbstractArray{Float64, 1}, gpm::GPModel)
</code></pre>

<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/gp.jl#L171-L173' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.gp_loglikelihood-Tuple{GpAbc.GPModel}' href='#GpAbc.gp_loglikelihood-Tuple{GpAbc.GPModel}'>#</a>
<strong><code>GpAbc.gp_loglikelihood</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>gp_loglikelihood(gpm::GPModel)
</code></pre>

<p>Compute the log likelihood function, based on the kernel and training data specified in <code>gpm</code>.</p>
<p>
<script type="math/tex; mode=display">
log p(y \vert X, \theta) = - \frac{1}{2}(y^TK^{-1}y + log \vert K \vert + n log 2 \pi)
</script>
</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/gp.jl#L161-L168' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.gp_regression-Tuple{GpAbc.GPModel}' href='#GpAbc.gp_regression-Tuple{GpAbc.GPModel}'>#</a>
<strong><code>GpAbc.gp_regression</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>gp_regression(gpm::GPModel; &lt;optional keyword arguments&gt;)
</code></pre>

<p>Run the Gaussian Process Regression.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>gpm</code>: the <a href=".#GpAbc.GPModel"><code>GPModel</code></a>, that contains the training data (x and y), the kernel, the hyperparameters and the test data for running the regression.</li>
<li><code>test_x</code>: if specified, overrides the test x in <code>gpm</code>. Size $m \times d$.</li>
<li><code>log_level::Int</code> (optional): log level. Default is <code>0</code>, which is no logging at all. <code>1</code> makes <code>gp_regression</code> print basic information to standard output.</li>
<li><code>full_covariance_matrix::Bool</code> (optional): whether we need the full covariance matrix, or just the variance vector. Defaults to <code>false</code> (i.e. just the variance).</li>
<li><code>batch_size::Int</code> (optional): If <code>full_covariance_matrix</code> is set to <code>false</code>, then the mean and variance vectors will be computed in batches of this size, to avoid allocating huge matrices. Defaults to 1000.</li>
<li><code>observation_noise::Bool</code> (optional): whether the observation noise (with variance $\sigma_n^2$) should be included in the output variance. Defaults to <code>true</code>.</li>
</ul>
<p><strong>Return</strong></p>
<p>A tuple of <code>(mean, var)</code>. <code>mean</code> is a mean vector of the output multivariate Normal distribution, of size $m$. <code>var</code> is either the covariance matrix of size $m \times m$, or a variance vector of size $m$, depending on <code>full_covariance_matrix</code> flag.</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/gp.jl#L192-L215' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.gp_regression-Tuple{Union{AbstractArray{Float64,1}, AbstractArray{Float64,2}},GpAbc.GPModel}' href='#GpAbc.gp_regression-Tuple{Union{AbstractArray{Float64,1}, AbstractArray{Float64,2}},GpAbc.GPModel}'>#</a>
<strong><code>GpAbc.gp_regression</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>gp_regression(test_x::Union{AbstractArray{Float64, 1}, AbstractArray{Float64, 2}},
    gpem::GPModel; &lt;optional keyword arguments&gt;)
</code></pre>

<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/gp.jl#L220-L223' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.gp_train-Tuple{GpAbc.GPModel}' href='#GpAbc.gp_train-Tuple{GpAbc.GPModel}'>#</a>
<strong><code>GpAbc.gp_train</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>gp_train(gpm::GPModel; &lt;optional keyword arguments&gt;)
</code></pre>

<p>Find Maximum Likelihood Estimate of Gaussian Process hyperparameters by maximising <a href=".#GpAbc.gp_loglikelihood-Tuple{AbstractArray{Float64,1},GpAbc.GPModel}"><code>gp_loglikelihood</code></a>, using <a href="http://julianlsolvers.github.io/Optim.jl/stable/"><code>Optim</code></a> package.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>gpm</code>: the <a href=".#GpAbc.GPModel"><code>GPModel</code></a>, that contains the training data (x and y), the kernel and the starting hyperparameters that will be used for optimisation.</li>
<li><code>optimisation_solver_type::Type{&lt;:Optim.Optimizer}</code> (optional): the solver to use. If not given, then <code>ConjugateGradient</code> will be used for kernels that have gradient implementation, and <code>NelderMead</code> will be used for those that don't.</li>
<li><code>hp_lower::AbstractArray{Float64, 1}</code> (optional): the lower boundary for box optimisation. Defaults to $e^{-10}$ for all hyperparameters.</li>
<li><code>hp_upper::AbstractArray{Float64, 1}</code> (optional): the upper boundary for box optimisation. Defaults to $e^{10}$ for all hyperparameters.</li>
<li><code>log_level::Int</code> (optional): log level. Default is <code>0</code>, which is no logging at all. <code>1</code> makes <code>gp_train</code> print basic information to standard output. <code>2</code> switches <code>Optim</code> logging on, in addition to <code>1</code>.</li>
</ul>
<p><strong>Return</strong></p>
<p>The list of all hyperparameters, including the standard deviation of the measurement noise $\sigma_n$. Note that after this function returns, the hyperparameters of <code>gpm</code> will be set to the optimised value, and there is no need to call <a href=".#GpAbc.set_hyperparameters-Tuple{GpAbc.GPModel,AbstractArray{Float64,1}}"><code>set_hyperparameters</code></a> once again.</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/gp_optimisation.jl#L17-L42' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.rejection_abc-Tuple{Function,Function}' href='#GpAbc.rejection_abc-Tuple{Function,Function}'>#</a>
<strong><code>GpAbc.rejection_abc</code></strong> &mdash; <em>Method</em>.</p>
<p>test_prior - A no-argument function, returns a sample from test prior, that is ready to be fed into summary_statistics_function. Returns an array of size (n, d), n rows and d columns, where n is the number of points, and d is the dimentionality of the data</p>
<p>summary_statistics_function - A function that takes in a test sample, and returns a 1D vector of summary statistics. In our case this returns a vector of norms to the observed data, as emulated by the GP.</p>
<p>observed_summary_statistic - Summary statistic of the observed data. In our case this is zero</p>
<p>See abc_test.jl for usage example</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/abc.jl#L1-L14' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}' href='#GpAbc.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}'>#</a>
<strong><code>GpAbc.scaled_squared_distance</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>scaled_squared_distance(log_ell::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, z::AbstractArray{Float64, 2})
</code></pre>

<p>Compute the scaled squared distance between <code>x</code> and <code>z</code>:</p>
<p>
<script type="math/tex; mode=display">
r_{ij} = \sum_{k=1}^d\frac{(x_{ik}-z_{jk})^2}{l_k^2}
</script>
</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>x, z</code>: Input data, reshaped into 2-d arrays. <code>x</code> must have dimensions $n \times d$; <code>z</code> must have dimensions $m \times d$.</li>
<li><code>log_ell</code>: logarithm of length scale(s). Can either be an array of size one (isotropic), or an array of size <code>d</code> (ARD)</li>
</ul>
<p><strong>Return</strong></p>
<p>An $n \times m$ matrix of scaled squared distances</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/scaled_squared_distance.jl#L1-L18' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.scaled_squared_distance_grad-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2},AbstractArray{Float64,2}}' href='#GpAbc.scaled_squared_distance_grad-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2},AbstractArray{Float64,2}}'>#</a>
<strong><code>GpAbc.scaled_squared_distance_grad</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>scaled_squared_distance_grad(log_ell::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, z::AbstractArray{Float64, 2}, R::AbstractArray{Float64, 2})
</code></pre>

<p>Return the gradient of the <a href=".#GpAbc.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a> function with respect to logarigthms of length scales, based on the provided direction matrix.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><code>x, z</code>: Input data, reshaped into 2-d arrays. <code>x</code> must have dimensions $n \times d$; <code>z</code> must have dimensions $m \times d$.</li>
<li><code>log_ell</code>: logarithm of length scale(s). Can either be an array of size one (isotropic), or an array of size <code>d</code> (ARD)</li>
<li><code>R</code> the direction matrix, $n \times m$. This can be used to compute the gradient of a function that depends on <a href=".#GpAbc.scaled_squared_distance-Tuple{AbstractArray{Float64,1},AbstractArray{Float64,2},AbstractArray{Float64,2}}"><code>scaled_squared_distance</code></a> via the chain rule.</li>
</ul>
<p><strong>Return</strong></p>
<p>A vector of size <code>length(log_ell)</code>, whose $k$'th element is equal to</p>
<p>
<script type="math/tex; mode=display">
\text{tr}(R \frac{\partial K}{\partial l_k})
</script>
</p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/kernels/scaled_squared_distance.jl#L43-L63' class='documenter-source'>source</a><br></p>
<p><a id='GpAbc.set_hyperparameters-Tuple{GpAbc.GPModel,AbstractArray{Float64,1}}' href='#GpAbc.set_hyperparameters-Tuple{GpAbc.GPModel,AbstractArray{Float64,1}}'>#</a>
<strong><code>GpAbc.set_hyperparameters</code></strong> &mdash; <em>Method</em>.</p>
<pre><code>set_hyperparameters(gpm::GPModel, hypers::AbstractArray{Float64, 1})
</code></pre>

<p>Set the hyperparameters of the <a href=".#GpAbc.GPModel"><code>GPModel</code></a></p>
<p><a target='_blank' href='https://github.com/tanhevg/GpAbc.jl/blob/8c260ef5b2c52e5b9a784c66ff85009dde6dc64b/src/gp.jl#L135-L139' class='documenter-source'>source</a><br></p>
              
            </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/tanhevg/GpAbc.jl/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
      
    </span>
</div>
    <script>var base_url = '.';</script>
    <script src="./js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
      <script src="./assets/mathjaxhelper.js"></script>
      <script src="./search/require.js"></script>
      <script src="./search/search.js"></script>

</body>
</html>

<!--
MkDocs version : 0.17.3
Build Date UTC : 2018-05-17 17:25:44
-->
