<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Examples · GpAbc.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>GpAbc.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li class="current"><a class="toctext" href>Examples</a><ul class="internal"></ul></li><li><a class="toctext" href="../reference/">Reference</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Examples</a></li></ul><a class="edit-page" href="https://github.com/tanhevg/GpAbc.jl/blob/master/docs/src/examples.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Examples</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Examples-1" href="#Examples-1">Examples</a></h1><ul><li><p><a href="#example-1-1">Basic Gaussian Process Regression</a></p></li><li><p><a href="#example-2-1">Optimising Hyperparameters for GP Regression</a></p></li><li><p><a href="#example-3-1">Advanced Usage of gp_train</a></p></li><li><p><a href="#example-4-1">Using a Custom Kernel</a></p></li></ul><h3><a class="nav-anchor" id="example-1-1" href="#example-1-1">Basic Gaussian Process Regression</a></h3><pre><code class="language-julia">using GpAbc, Distributions, PyPlot

# prepare the data
n = 30
f(x) = x.ˆ2 + 10 * sin.(x) # the latent function

training_x = sort(rand(Uniform(-10, 10), n))
training_y = f(training_x)
training_y += 20 * (rand(n) - 0.5) # add some noise
test_x = collect(linspace(min(training_x...), max(training_x...), 1000))

 # SquaredExponentialIsoKernel is used by default
gpm = GPModel(training_x, training_y)

# pretend we know the hyperparameters in advance
# σ_f = 37.08; l = 1.0; σ_n = 6.58. See SquaredExponentialIsoKernel documentation for details
set_hyperparameters(gpm, [37.08, 1.0, 6.58])
(test_y, test_var) = gp_regression(test_x, gpm)

plot(test_x, [test_y f(test)]) # ... and more sophisticated plotting</code></pre><h3><a class="nav-anchor" id="example-2-1" href="#example-2-1">Optimising Hyperparameters for GP Regression</a></h3><p>Based on <a href="#example-1-1">Basic Gaussian Process Regression</a>, but with added optimisation of hyperparameters:</p><pre><code class="language-julia">using GaussProABC

# prepare the data ...

gpm = GPModel(training_x, training_y)

 # by default, the optimiser will start with all hyperparameters set to 1,
 # constrained between exp(-10) and exp(10)
theta_mle = gp_train(gpm)

# optimised hyperparameters are stored in gpm, so no need to pass them again
(test_y, test_var) = gp_regression(test_x, gpm)</code></pre><h3><a class="nav-anchor" id="example-3-1" href="#example-3-1">Advanced Usage of gp_train</a></h3><pre><code class="language-julia">using GpAbc, Optim, Distributions

function gp_train_advanced(gpm::GPModel, attempts::Int)
    # Initialise the bounds, with special treatment for the second hyperparameter
    p = get_hyperparameters_size(gpm)
    bound_template = ones(p)
    upper_bound = bound_template * 10
    upper_bound[2] = 2
    lower_bound = bound_template * -10
    lower_bound[2] = -1

    # Starting point will be sampled from a Multivariate Uniform distribution
    start_point_distr = MvUniform(lower_bound, upper_bound)

    # Run several attempts of training and store the
    # minimiser hyperparameters and the value of log likelihood function
    hypers = Array{Float64}(attempts, p)
    likelihood_values = Array{Float64}(attempts)
    for i=1:attempts
        set_hyperparameters(gpm, exp.(rand(start_point_distr)))
        hypers[i, :] = gp_train(gpm,
            optimisation_solver_type=SimulatedAnnealing, # note the solver type
            hp_lower=lower_bound, hp_upper=upper_bound, log_level=1)
        likelihood_values[i] = gp_loglikelihood(gpm)
    end
    # Retain the hyperparameters where the maximum log likelihood function is attained
    gpm.gp_hyperparameters = hypers[indmax(likelihood_values), :]
end</code></pre><h3><a class="nav-anchor" id="example-4-1" href="#example-4-1">Using a Custom Kernel</a></h3><p>The methods below should be implemented for the custom kernel, unless indicated as optional. Please see reference documentation for detailed description of each method and parameter.</p><pre><code class="language-julia">using GpAbc
import GpAbc.covariance, GpAbc.get_hyperparameters_size

&quot;&quot;&quot;
   This is the new kernel that we are adding
&quot;&quot;&quot;
type MyCustomkernel &lt;: AbstractGPKernel

    # optional cache of matrices that could be re-used between calls to
    # covariance_training and covariance_grad, keyed by hyperparameters
    cache::MyCustomCache
end

&quot;&quot;&quot;
    Report the number of hyperparameters required by the new kernel
&quot;&quot;&quot;
function get_hyperparameters_size(ker::MyCustomkernel, training_data::AbstractArray{Float64, 2})
    # ...
end

&quot;&quot;&quot;
    Covariance function of the new kernel.

    Return the covariance matrix. Assuming x is an n by d matrix, and z is an m by d matrix,
    this should return an n by m matrix. Use `scaled_squared_distance` helper function here.
&quot;&quot;&quot;
function covariance(ker::MyCustomkernel, log_theta::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, z::AbstractArray{Float64, 2})
    # ...
end

&quot;&quot;&quot;
    Optional speedup of `covariance` function, that is invoked when the calling code is
    only interested in variance (i.e. diagonal elements of the covariance) of the kernel.
&quot;&quot;&quot;
function covariance_diagonal(ker::MyCustomkernel, log_theta::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2})
    # ...
end

&quot;&quot;&quot;
   Optional speedup of `covariance` function that is invoked during training of the GP.
   Intermediate matrices that are re-used between this function and `covariance_grad` could
   be cached in `ker.cache`
&quot;&quot;&quot;
function covariance_training(ker::MyCustomkernel, log_theta::AbstractArray{Float64, 1},
    training_x::AbstractArray{Float64, 2})
    # ...
end

&quot;&quot;&quot;
    Optional gradient of `covariance` function with respect to hyperparameters, required
    for optimising with `ConjugateGradient` method. If not provided, `NelderMead` optimiser
    will be used.

    Use `scaled_squared_distance_grad` helper function here.
&quot;&quot;&quot;
function covariance_grad(ker::MyCustomkernel, log_theta::AbstractArray{Float64, 1},
    x::AbstractArray{Float64, 2}, R::AbstractArray{Float64, 2})
    # ...
end

gpm = GPModel(training_x, training_y, MyCustomkernel())
theta_mle = gp_train(gpm)
(test_y, test_var) = gp_regression(test_x, gpm)</code></pre><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Home</span></a><a class="next" href="../reference/"><span class="direction">Next</span><span class="title">Reference</span></a></footer></article></body></html>
